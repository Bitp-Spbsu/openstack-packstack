diff -uNr packstack-2014.1.1dev1238/packstack/plugins/cinder_250.py packstack-2014.1.1dev1238p/packstack/plugins/cinder_250.py
--- packstack-2014.1.1dev1238/packstack/plugins/cinder_250.py	2014-08-13 11:01:15.000000000 +0300
+++ packstack-2014.1.1dev1238p/packstack/plugins/cinder_250.py	2014-10-16 00:43:02.044303081 +0300
@@ -65,7 +65,7 @@
          "USAGE": ("The Cinder backend to use, valid options are: lvm, "
                    "gluster, nfs"),
          "PROMPT": "Enter the Cinder backend to be configured",
-         "OPTION_LIST": ["lvm", "gluster", "nfs", "vmdk"],
+         "OPTION_LIST": ["lvm", "gluster", "nfs", "vmdk", "ceph"],
          "VALIDATORS": [validators.validate_options],
          "DEFAULT_VALUE": "lvm",
          "MASK_INPUT": False,
@@ -346,6 +346,8 @@
         manifestdata += getManifestTemplate("cinder_nfs.pp")
     elif config['CONFIG_CINDER_BACKEND'] == "vmdk":
         manifestdata += getManifestTemplate("cinder_vmdk.pp")
+    elif config['CONFIG_CINDER_BACKEND'] == "ceph":
+        manifestdata += getManifestTemplate("cinder_ceph.pp")
     if config['CONFIG_CEILOMETER_INSTALL'] == 'y':
         manifestdata += getManifestTemplate('cinder_ceilometer.pp')
     if config['CONFIG_SWIFT_INSTALL'] == 'y':
diff -uNr packstack-2014.1.1dev1238/packstack/plugins/nova_300.py packstack-2014.1.1dev1238p/packstack/plugins/nova_300.py
--- packstack-2014.1.1dev1238/packstack/plugins/nova_300.py	2014-08-13 11:01:15.000000000 +0300
+++ packstack-2014.1.1dev1238p/packstack/plugins/nova_300.py	2014-10-16 00:42:57.561359434 +0300
@@ -512,6 +512,10 @@
                 config['CONFIG_CINDER_INSTALL'] == 'y' and
                 config['CONFIG_CINDER_BACKEND'] == 'nfs'):
             manifestdata += getManifestTemplate("nova_nfs.pp")
+        if (config['CONFIG_VMWARE_BACKEND'] != 'y' and
+                config['CONFIG_CINDER_INSTALL'] == 'y' and
+                config['CONFIG_CINDER_BACKEND'] == 'ceph'):
+            manifestdata += getManifestTemplate("nova_compute_ceph.pp")
         manifestfile = "%s_nova.pp" % host
 
         nova_config_options = NovaConfig()
diff -uNr packstack-2014.1.1dev1238/packstack/puppet/templates/cinder_ceph.pp packstack-2014.1.1dev1238p/packstack/puppet/templates/cinder_ceph.pp
--- packstack-2014.1.1dev1238/packstack/puppet/templates/cinder_ceph.pp	1970-01-01 03:00:00.000000000 +0300
+++ packstack-2014.1.1dev1238p/packstack/puppet/templates/cinder_ceph.pp	2014-10-16 00:42:47.311488278 +0300
@@ -0,0 +1,240 @@
+Exec { path => "/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin" }
+
+$admin_node = "${hostname}" 
+$nova_node = "%(CONFIG_COMPUTE_HOSTS)s"
+$rdo_node="$(dig +short -x ${nova_node} | rev | cut -c 2- | rev | tr -d \"\n\")"
+$current_dir = "/root"
+$basearch = "x86_64"
+
+yumrepo { "ceph":
+    descr => "Ceph packages for ${basearch}",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/${basearch}",
+    priority => "1",
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-source":
+    descr => "Ceph source packages",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/SRPMS",
+    priority => 1,
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-noarch":
+    descr => "Ceph noarch packages",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/noarch",
+    priority => 1,
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-extras":
+    descr => "Ceph Extras",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/${basearch}",
+    priority => 2,
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-qemu-source":
+    descr => "Ceph Extras Sources",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/packages/ceph-extras/rpm/centos6/SRPMS",
+    priority => 2,
+    gpgcheck => 1,
+    ensure => present,
+}
+
+#echo " --- Time sync'ing"
+package { "ntp":
+    ensure => installed,
+}
+package { "ntpdate":
+    ensure => installed,
+}
+package { "ntp-doc":
+    ensure => installed,
+}
+exec { "ntpdate":
+    command => "ntpdate -b 0.ua.pool.ntp.org",
+    require => Package["ntpdate"],
+}
+
+#echo " --- Checking ssh server"
+package { "openssh-server":
+    ensure => installed,
+}
+
+#echo " --- Creating ceph user"
+user { "ceph":
+    ensure => present,
+}
+#passwd ceph
+#echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
+#sudo chmod 0440 /etc/sudoers.d/ceph
+#su ceph
+#ssh-keygen
+#ssh-copy-id ceph@194.44.37.125
+
+# ???
+#cinder    ALL=(ALL) NOPASSWD: ALL
+
+
+#echo " --- Installing RDO"
+package { "rdo-release":
+    ensure => installed,
+    source => "https://rdo.fedorapeople.org/rdo-release.rpm",
+}
+
+exec { "yum-update":
+    command => "yum update -y",
+    require => Package["rdo-release"],
+}->
+package { "ceph-libs":
+    ensure => absent,
+}->
+package { "ceph-deploy":
+    ensure => installed,
+}
+
+#echo " --- Deploying new node"
+exec { "ceph-deploy-new-node":
+    command => "ceph-deploy new ${rdo_node}",
+    require => Package["ceph-libs"],
+    timeout => 1800,
+}
+
+#if $rdo_node == $admin_node {
+if $ipaddress == $nova_host {
+    $install_nodes = $admin_node
+}
+else {
+    $install_nodes = [ "${admin_node}", " ${rdo_node}" ]
+}
+#echo " --- Installing CEPH"
+exec { "ceph-deploy-install":
+    command => "ceph-deploy install ${install_nodes}",
+#    command => "ceph-deploy install ${admin_node}",
+    require => [ Package["ceph-libs"],
+                 Exec["ceph-deploy-new-node"] ],
+    timeout => 1800,
+}->
+#echo " --- Modifying ceph.conf"
+file { "/root/ceph.conf":
+    ensure => present,
+    require => Exec["ceph-deploy-install"],
+} -> 
+file_line { "Append a line to /root/ceph.conf":
+    path => "/root/ceph.conf",  
+    line => 
+"osd pool default size = 1
+public network = 194.44.37.0/24
+cluster network = 10.1.1.0/24",
+}
+
+#???
+/*file { "/etc/ceph/ceph.conf":
+    ensure => present,
+    source => "/root/ceph.conf",
+    require => File["/root/ceph.conf"],
+}*/
+
+#echo " --- Adding the initial monitor and gathering the keys"
+exec { "ceph-deploy-monitor-create":
+    command => "ceph-deploy --overwrite-conf mon create ${rdo_node}",
+    require => Exec["ceph-deploy-install"],
+}
+exec { "ceph-deploy-monitor-gatherkeys":
+    command => "ceph-deploy gatherkeys ${rdo_node}",
+    require => Exec["ceph-deploy-monitor-create"],
+    creates => [ "${current_dir}/ceph.client.admin.keyring",
+                 "${current_dir}/ceph.bootstrap-osd.keyring",
+                 "${current_dir}/ceph.bootstrap-mds.keyring" ],
+}
+
+#echo " --- Creating OSD"
+exec { "ceph-osd-prepare":
+    command => "ceph-deploy --overwrite-conf osd prepare ${rdo_node}:/var/local/osd0",
+    require => Exec["ceph-deploy-monitor-gatherkeys"],
+}
+exec { "ceph-deploy-osd":
+    command => "ceph-deploy osd activate ${rdo_node}:/var/local/osd0",
+    require => Exec["ceph-osd-prepare"],
+}
+
+#echo " --- Copying the configuration file and admin key"
+exec { "ceph-deploy-admin":
+    command => "ceph-deploy --overwrite-conf admin ${admin_node} ${rdo_node}",
+    require => [ Exec["ceph-deploy-monitor-gatherkeys"],
+                 Exec["ceph-deploy-osd"] ],
+}->
+file { "/etc/ceph/ceph.client.admin.keyring":
+    mode => "+r",    
+}
+
+#echo " --- Adding a Metadata Server"
+exec {'ceph-deploy-mds':
+    command => "ceph-deploy --overwrite-conf mds create ${rdo_node}",
+    require => Exec["ceph-deploy-monitor-gatherkeys"],
+}
+
+file_line { "Append2 a line to /root/ceph.conf":
+    path => "/root/ceph.conf",  
+    line => 
+"[client.images]
+keyring = /etc/ceph/ceph.client.images.keyring
+
+[client.volumes]
+keyring = /etc/ceph/ceph.client.volumes.keyring
+
+[client.backups]
+keyring = /etc/ceph/ceph.client.backups.keyring",
+    require => [ File["/root/ceph.conf"],
+                 Exec["ceph-deploy-osd"] ],
+}->
+exec { "ceph-config-push":
+    command => "ceph-deploy --overwrite-conf config push ${rdo_node}",
+}
+
+#echo " --- Solving ceilometer-api dateutil issue"
+package { "python-dateutil":
+    ensure => latest,
+    provider => "pip",
+}
+
+firewall { "00000 Ceph monitor on port 6789":
+  chain    => "INPUT",
+#  iniface  => "eth1",
+  proto => "tcp",
+#  source   => "10.1.1.0/24",
+  dport => "6789",
+  action => "accept",
+  notify => Exec["iptables-save"]
+}
+
+firewall { "00001 Ceph OSDs on port 6800:7100":
+  chain    => "INPUT",
+#  iniface  => "eth1",
+  proto => "tcp",
+#  source   => "10.1.1.0/24",
+  dport => "6800-7100",
+  action => "accept",
+  notify => Exec["iptables-save"]
+}
+
+exec { "iptables-save":
+  command  => "/sbin/iptables-save > /etc/sysconfig/iptables",
+  refreshonly => true,
+}
+
diff -uNr packstack-2014.1.1dev1238/packstack/puppet/templates/nova_compute_ceph.pp packstack-2014.1.1dev1238p/packstack/puppet/templates/nova_compute_ceph.pp
--- packstack-2014.1.1dev1238/packstack/puppet/templates/nova_compute_ceph.pp	1970-01-01 03:00:00.000000000 +0300
+++ packstack-2014.1.1dev1238p/packstack/puppet/templates/nova_compute_ceph.pp	2014-10-16 00:42:51.082440877 +0300
@@ -0,0 +1,242 @@
+Exec { path => "/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin" }
+
+$rdo_node = "${hostname}"
+$current_dir = "/root"
+$basearch = "x86_64"
+
+#echo " --- Checking libvirt"
+/*package { "libvirt":
+    ensure => installed,
+}
+service { "libvirtd":
+    ensure => running,
+    enable => true,
+    path => "/etc/init.d/libvirtd",
+    require => Package["libvirt"],
+}*/
+/*
+yumrepo { "ceph":
+    descr => "Ceph packages for ${basearch}",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/${basearch}",
+    priority => "1",
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-source":
+    descr => "Ceph source packages",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/SRPMS",
+    priority => 1,
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-noarch":
+    descr => "Ceph noarch packages",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/noarch",
+    priority => 1,
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-extras":
+    descr => "Ceph Extras",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/rpm-firefly/el6/${basearch}",
+    priority => 2,
+    gpgcheck => 1,
+    ensure => present,
+}
+
+yumrepo { "ceph-qemu-source":
+    descr => "Ceph Extras Sources",
+    gpgkey => "https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc",
+    enabled => 1,
+    baseurl => "http://ceph.com/packages/ceph-extras/rpm/centos6/SRPMS",
+    priority => 2,
+    gpgcheck => 1,
+    ensure => present,
+}
+*/
+#echo " --- Time sync'ing"
+package { "ntp":
+    ensure => installed,
+}
+package { "ntpdate":
+    ensure => installed,
+}
+package { "ntp-doc":
+    ensure => installed,
+}
+exec { "ntpdate":
+    command => "ntpdate -b 0.ua.pool.ntp.org",
+    require => Package["ntpdate"],
+}
+
+#echo " --- Checking ssh server"
+package { "openssh-server":
+    ensure => installed,
+}
+
+#echo " --- Creating ceph user"
+user { "ceph":
+    ensure => present,
+}
+#passwd ceph
+#echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
+#sudo chmod 0440 /etc/sudoers.d/ceph
+#su ceph
+#ssh-keygen
+#ssh-copy-id ceph@194.44.37.125
+
+# ???
+#cinder    ALL=(ALL) NOPASSWD: ALL
+
+
+#echo " --- Installing RDO"
+package { "rdo-release":
+    ensure => installed,
+    source => "https://rdo.fedorapeople.org/rdo-release.rpm",
+}->
+package { "ceph-libs":
+    ensure => absent,
+}->
+file { "/etc/ceph/ceph.conf":
+    ensure => present,
+}
+
+#echo " --- Create pools"
+$poolname1 = "images"
+$poolname2 = "volumes"
+$poolname3 = "backups"
+
+package { "ceph": ensure => installed}
+file { "/var/local/osd0/ready": }
+exec { "ceph-create-osd-pool":
+    command => "ceph osd pool create ${poolname1} 128 ; ceph osd pool create ${poolname2} 128 ; ceph osd pool create ${poolname3} 128",
+    require => Package["ceph"],  
+}    
+
+#echo " --- Create a keyring and user for images, volumes and backups"
+$keyring_path = "/etc/ceph"
+
+exec { "ceph-key-${poolname1}":
+    command => "ceph-authtool --create-keyring ${keyring_path}/ceph.client.${poolname1}.keyring",
+    require => Exec["ceph-create-osd-pool"],
+}->
+file { "/etc/ceph/ceph.client.${poolname1}.keyring":
+    mode => "+r",
+}->
+exec { "ceph-authtool-${poolname1}":
+    command => "ceph-authtool ${keyring_path}/ceph.client.${poolname1}.keyring -n client.${poolname1} --gen-key ; ceph-authtool -n client.${poolname1} --cap mon 'allow r' --cap osd 'allow class-read object_prefix rbd_children, allow rwx  pool=${poolname1}' ${keyring_path}/ceph.client.${poolname1}.keyring ; ceph auth add client.${poolname1} -i ${keyring_path}/ceph.client.${poolname1}.keyring",
+}
+
+exec { "ceph-key-${poolname2}":
+    command => "ceph-authtool --create-keyring ${keyring_path}/ceph.client.${poolname2}.keyring",
+    require => Exec["ceph-create-osd-pool"],
+}->
+file { "/etc/ceph/ceph.client.${poolname2}.keyring":
+    mode => "+r",
+}->
+exec { "ceph-authtool-${poolname2}":
+    command => "ceph-authtool ${keyring_path}/ceph.client.${poolname2}.keyring -n client.${poolname2} --gen-key ; ceph-authtool -n client.${poolname2} --cap mon 'allow r' --cap osd 'allow class-read object_prefix rbd_children, allow rwx  pool=${poolname2}' ${keyring_path}/ceph.client.${poolname2}.keyring ; ceph auth add client.${poolname2} -i ${keyring_path}/ceph.client.${poolname2}.keyring",
+}
+
+exec { "ceph-key-${poolname3}":
+    command => "ceph-authtool --create-keyring ${keyring_path}/ceph.client.${poolname3}.keyring",
+    require => Exec["ceph-create-osd-pool"],
+}->
+file { "/etc/ceph/ceph.client.${poolname3}.keyring":
+    mode => "+r",
+}->
+exec { "ceph-authtool-${poolname3}":
+    command => "ceph-authtool ${keyring_path}/ceph.client.${poolname3}.keyring -n client.${poolname3} --gen-key ; ceph-authtool -n client.${poolname3} --cap mon 'allow r' --cap osd 'allow class-read object_prefix rbd_children, allow rwx  pool=${poolname3}' ${keyring_path}/ceph.client.${poolname3}.keyring ; ceph auth add client.${poolname3} -i ${keyring_path}/ceph.client.${poolname3}.keyring",
+}
+
+#echo " --- Configuring Libvirt"
+augeas { "sudo/requiretty":
+	incl    => "/etc/sudoers",
+	lens    => "Sudoers.lns",
+	changes => [
+		"ins #comment before Defaults[requiretty]",
+		"set #comment[following-sibling::Defaults/requiretty][last()] 'Defaults requiretty'",
+		"rm Defaults/requiretty",
+		"rm Defaults[count(*) = 0]",
+	],
+	onlyif => "match Defaults/requiretty size > 0",
+	before => Exec["client-volumes-key"],
+}
+exec { "client-volumes-key":
+    command => "ceph auth get-key client.volumes | tee client.volumes.key",
+    require => [ Exec["ceph-authtool-volumes"],
+		 ],
+#    before => Exec["virsh"],
+    creates => "/root/client.volumes.key",
+}
+
+file { "/root/secret.xml":
+  ensure => present,
+  content => 
+"<secret ephemeral='no' private='no'>
+<usage type='ceph'>
+  <name>client.volumes secret</name>
+</usage>
+</secret>",
+}
+
+exec { "virsh":
+    command => "virsh secret-define --file secret.xml &> virsh.result; cat virsh.result | egrep -o '[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}' > rbd.secret.uuid",
+    require => [ #Service["libvirt"],
+                 File["/root/secret.xml"],
+                 Exec["client-volumes-key"] ],
+}
+
+exec { "virsh2":
+    command => "virsh secret-set-value --secret `/bin/cat rbd.secret.uuid` --base64 `/bin/cat client.volumes.key`",
+    require => Exec ["virsh"],
+}
+->
+file { ["/root/client.volumes.key",
+        "/root/virsh.result",
+        "/root/rbd.secret.uuid"]:
+    ensure => absent,
+}
+
+exec { "ceph-osd-libvirt-pool":
+    command => "ceph osd pool create libvirt-pool 128 128 ; ceph auth get-or-create client.libvirt mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=libvirt-pool'",
+    require =>  Package["ceph"],
+                 #Service["libvirt"] ],
+}
+
+firewall { "00000 Ceph monitor on port 6789":
+  chain    => "INPUT",
+#  iniface  => "eth1",
+  proto => "tcp",
+#  source   => "10.1.1.0/24",
+  dport => "6789",
+  action => "accept",
+  notify => Exec["iptables-save"]
+}
+
+firewall { "00001 Ceph OSDs on port 6800:7100":
+  chain    => "INPUT",
+#  iniface  => "eth1",
+  proto => "tcp",
+#  source   => "10.1.1.0/24",
+  dport => "6800-7100",
+  action => "accept",
+  notify => Exec["iptables-save"]
+}
+
+exec { "iptables-save":
+  command  => "/sbin/iptables-save > /etc/sysconfig/iptables",
+  refreshonly => true,
+}
+
